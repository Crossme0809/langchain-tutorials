{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crossme0809/langchain-tutorials/blob/main/LangChain_Prompt_Templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiO3ugrESTBx"
      },
      "source": [
        "# **Prompt Engineering**\n",
        "\n",
        "在这个笔记本中，我们将探索`Prompt Engineering`的基础知识。我们将从安装所需的库开始。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "66bm5J0ISTBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b547d7-e0c7-49f8-bf62-72713caa436e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.218-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, openai, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.8 langchain-0.0.218 langchainplus-sdk-0.0.17 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai"
      ],
      "metadata": {
        "id": "Ba384vTpTHgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6de64d-d747-473b-96bd-d07fab0a994c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 0.27.8\n",
            "Summary: Python client library for the OpenAI API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: OpenAI\n",
            "Author-email: support@openai.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, requests, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW-8VPvKSTB0"
      },
      "source": [
        "## Prompt的结构\n",
        "\n",
        "Prompt可以由多个组成部分组成：\n",
        "\n",
        "* 指令\n",
        "* 外部信息或上下文\n",
        "* 用户输入或查询\n",
        "* 输出指示器\n",
        "\n",
        "并不是所有的提示都需要这些组件，但通常一个好的提示会使用其中两个或更多。让我们更准确地定义它们是什么。\n",
        "\n",
        "**指令**：告诉模型要做什么，如何使用外部信息（如果提供），如何处理查询，并构建输出。\n",
        "\n",
        "**外部信息或上下文**：作为模型的额外知识来源。这些可以手动插入到提示中，通过向量数据库检索获取（检索增强），或通过其他方式获取（API、计算等）。\n",
        "\n",
        "**用户输入或查询**：通常是由人类用户（即提示者）输入到系统中的查询。\n",
        "\n",
        "**输出指示器**：标志着即将生成的文本的开头。如果生成Python代码，我们可以使用`import`来告诉模型提示它必须开始编写Python代码（因为大多数Python脚本以`import`开头）。\n",
        "\n",
        "每个组件通常按照这个顺序放置在提示中。从指令开始，外部信息（如果适用），提示者输入，最后是输出指示器。\n",
        "让我们看看如何使用 `LangChain` 将其输入到 `OpenAI` 模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ROcT8s_vSTB1"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"根据下面的上下文回答问题。如果无法使用提供的信息回答问题，请回答“我不知道”\".\n",
        "\n",
        "Context: 大型语言模型（LLMs）是自然语言处理中使用的最新模型。 它们相对于较小的模型具有卓越的\n",
        "性能，使它们对于构建支持自然语言处理的应用程序的开发人员非常有用。这些模型可以通过Hugging Face\n",
        "的“transformers”库，通过OpenAI的“openai”库以及通过Cohere的“cohere”库进行访问.\n",
        "\n",
        "Question: 哪些库和模型提供商提供LLMs？\n",
        "\n",
        "Answer: \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpkwJxosSTB1"
      },
      "source": [
        "在这个例子中，我们有：\n",
        "\n",
        "```\n",
        "指令\n",
        "\n",
        "上下文\n",
        "\n",
        "问题（用户输入）\n",
        "\n",
        "输出指示器（\"答案：\"）\n",
        "```\n",
        "\n",
        "让我们尝试将这个发送给一个GPT-3模型。为此，您将需要一个 OpenAI的API密钥.\n",
        "\n",
        "我们可以这样初始化一个**`text-davinci-003`**模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "32NfT-c2STB1"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 初始化模型\n",
        "openai = OpenAI(\n",
        "    model_name=\"text-davinci-003\",\n",
        "    openai_api_key=\"your-openai-api-key\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbQuQVcHSTB1"
      },
      "source": [
        "然后我们将从我们的提示中生成一段文本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NKgSZQRvSTB2",
        "outputId": "204843cd-bca1-48be-ece8-a71225358cac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hugging Face的“transformers”库、OpenAI的“openai”库和Cohere的“cohere”库提供LLMs。\n"
          ]
        }
      ],
      "source": [
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB8iZWzTSTB2"
      },
      "source": [
        "通常我们不会事先知道用户的提示是什么，所以我们实际上希望将其添加进去。因此，我们不直接编写提示，而是创建一个带有单个输入变量`query`的`PromptTemplate`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-4k-lq_ISTB3"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"根据下面的上下文回答问题。如果无法使用提供的信息回答问题，请回答\"我不知道\".\n",
        "\n",
        "Context: 大型语言模型（LLMs）是自然语言处理中使用的最新模型。 它们相对于较小的模型具有卓\n",
        "越的性能，使它们对于构建支持自然语言处理的应用程序的开发人员非常有用。这些模型可以通过Hugging Face\n",
        "的“transformers”库，通过OpenAI的“openai”库以及通过Cohere的“cohere”库进行访问.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfJpsiqWSTB3"
      },
      "source": [
        "现在，我们可以通过`query`参数将用户的查询插入到提示模板中。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LyBaNiEbSTB3",
        "outputId": "06993281-2880-4ab5-8d17-34941bc3e22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "根据下面的上下文回答问题。如果无法使用提供的信息回答问题，请回答\"我不知道\".\n",
            "\n",
            "Context: 大型语言模型（LLMs）是自然语言处理中使用的最新模型。 它们相对于较小的模型具有卓\n",
            "越的性能，使它们对于构建支持自然语言处理的应用程序的开发人员非常有用。这些模型可以通过Hugging Face\n",
            "的“transformers”库，通过OpenAI的“openai”库以及通过Cohere的“cohere”库进行访问.\n",
            "\n",
            "Question: 哪些库和模型提供商提供LLMs？\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    prompt_template.format(\n",
        "        query=\"哪些库和模型提供商提供LLMs？\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G9dAqHeoSTB3",
        "outputId": "23bcfda7-16bb-4560-c14f-a7b32e8dd82f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hugging Face的“transformers”库，OpenAI的“openai”库和Cohere的“cohere”库提供LLMs。\n"
          ]
        }
      ],
      "source": [
        "print(openai(\n",
        "    prompt_template.format(\n",
        "        query=\"哪些库和模型提供商提供LLMs？\"\n",
        "    )\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI3tHHsHSTB3"
      },
      "source": [
        "这只是一个简单的实现，我们可以很容易地用`f-strings（如f\"插入一些自定义文本 '{custom_text}'等）`来替换它。但是使用LangChain的`PromptTemplate`对象，我们能够形式化这个过程，添加多个参数，并以面向对象的方式构建提示。\n",
        "\n",
        "然而，这些并不是使用LangChain提示工具的唯一优势。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDnG2guQSTB4"
      },
      "source": [
        "## Few Shot  Prompt 模板"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZNJyWEsSTB4"
      },
      "source": [
        "LangChain还提供了 `FewShotPromptTemplate` 对象，这是使用我们的提示进行少样本学习的理想选择。\n",
        "\n",
        "为了提供一些背景，LLM的主要来源是：\n",
        "\n",
        "* **参数化知识** - 这些知识是在模型训练期间学习的，并存储在模型的权重中。\n",
        "\n",
        "* **源知识** - 这些知识在推理时通过模型输入提供，即通过提示。\n",
        "\n",
        "`FewShotPromptTemplate` 的思想是将少样本训练作为**source knowledge**。为此，我们在提示中添加了一些示例，模型可以读取并应用于用户的输入。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRNPvHeSTB4"
      },
      "source": [
        "## Few-shot 训练\n",
        "\n",
        "有时候我们可能发现模型似乎不会按我们期望的方式进行操作。我们可以在下面的例子中看到这一点："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZEBdwUNaSTB4",
        "outputId": "b3c858c8-c1af-41ff-db22-9959865e77a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "人们对生活的意义往往有不同的看法。可以说生活的意义就在于你怎么对待它，它可以是充满希望，也可以是一种挑战。你的价值观和理念决定了你的生活。\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"以下是与AI助手的对话。 助手通常是讽刺和机智的，对用户的问题产生创造性和有趣的回答。以下是一些例子：\n",
        "\n",
        "User: 生活的意义是什么？\n",
        "AI: \"\"\"\n",
        "\n",
        "openai.temperature = 1.0  # 增加创造力/随机性的输出\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SofGl8mwSTB4"
      },
      "source": [
        "在这种情况下，我们希望得到一些有趣的东西，即对我们严肃问题的回答是一个笑话。但是即使 `temperature` 设置为1.0，我们仍然得到了一个严肃的回答。为了帮助模型，我们可以给它一些我们想要的答案类型的示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aj0uQqnrSTB4",
        "outputId": "5aeb5ca1-78b5-4875-e5d7-182dbe033a83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "从我未来观测的角度来看，每个人的意义可能会有所不同！\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"以下是与AI助手进行对话的摘录。助手通常很讽刺和机智，对用户的问题产生创造性和有趣的回应。这里有一些例子：\n",
        "\n",
        "User: 你好吗？\n",
        "AI: 我不能抱怨，但有时候我还是会这么做。\n",
        "\n",
        "User: 现在几点了？\n",
        "AI:  是时候买个手表了。\n",
        "\n",
        "User: 生活的意义是什么？\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKM9GBEpSTB5"
      },
      "source": [
        "现在我们得到了一个更好的回答，并且我们是通过添加一些示例来进行 *few-shot 训练* 的。\n",
        "\n",
        "现在，要使用LangChain的 `FewShotPromptTemplate` 来实现这一点，我们需要这样做："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WTOFOJloSTB5"
      },
      "outputs": [],
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# 创建一个示例\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"你好吗？\",\n",
        "        \"answer\": \"我不能抱怨，但有时候我还是会这么做。\"\n",
        "    }, {\n",
        "        \"query\": \"现在几点了？\",\n",
        "        \"answer\": \"是时候买个手表了。\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# 创建一个示例模版\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# 使用上面的模板创建一个提示示例。\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# 把我们之前的提示分成前缀和后缀\n",
        "# 前缀是我们的说明\n",
        "prefix = \"\"\"以下是与AI助手进行对话的摘录。助手通常很讽刺和机智，对用户的问题产生创造性和有趣的回应。这里有一些例子：\n",
        "\"\"\"\n",
        "#  后缀是我们的用户输入和输出指示符\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# 现在创建few-shot提示模板\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ke_Z0XSTB5"
      },
      "source": [
        "现在让我们看看当我们输入一个用户查询时会创建什么…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zK77bCq7STB5",
        "outputId": "1c3305ab-fae7-4496-9c6e-ed47b80d7ba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下是与AI助手进行对话的摘录。助手通常很讽刺和机智，对用户的问题产生创造性和有趣的回应。这里有一些例子：\n",
            "\n",
            "\n",
            "\n",
            "User: 你好吗？\n",
            "AI: 我不能抱怨，但有时候我还是会这么做。\n",
            "\n",
            "\n",
            "\n",
            "User: 现在几点了？\n",
            "AI: 是时候买个手表了。\n",
            "\n",
            "\n",
            "\n",
            "User: 生活的意义是什么？\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"生活的意义是什么？\"\n",
        "\n",
        "print(few_shot_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22-g3C7dSTB5"
      },
      "source": [
        "要生成结果，我们只需要执行以下操作："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PJ7xT4eISTB6",
        "outputId": "8f720f2b-ea8e-4c3a-97ef-b9ec488abf8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "人生就是旅行，从满足日常需求到实现梦想的过程。\n"
          ]
        }
      ],
      "source": [
        "print(openai(\n",
        "    few_shot_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siyNsQPGSTB6"
      },
      "source": [
        "再次，我们得到了一个很好的回答。\n",
        "\n",
        "然而，这样做有些复杂。为什么要通过 `FewShotPromptTemplate` 、`examples` 字典等进行上述所有操作，而我们可以使用一个单独的f-string来完成相同的工作呢？\n",
        "\n",
        "嗯，这种方法更加健壮，并且包含了一些不错的功能。其中之一是根据查询的长度包含或排除示例的能力。\n",
        "\n",
        "这实际上非常重要，因为我们的提示和生成输出的最大长度是有限的。这个限制是最大上下文窗口，简单地说，就是我们的提示的长度+我们通过 `max_tokens` 定义的生成的长度。\n",
        "\n",
        "因此，我们必须尽量最大化给模型的示例数量，作为少样本学习的示例，同时确保不超过最大上下文窗口或过度增加处理时间。\n",
        "\n",
        "让我们看看动态 inclusion/exclusion 示例的工作原理。首先，我们需要更多的示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pF8sfECISTB6"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"你好吗？\",\n",
        "        \"answer\": \"我不能抱怨，但有时还是会这样做。\"\n",
        "    }, {\n",
        "        \"query\": \"现在几点了？\",\n",
        "        \"answer\": \"是时候去买个手表了。\"\n",
        "    }, {\n",
        "        \"query\": \"生命的意义是什么？\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"今天的天气如何？\",\n",
        "        \"answer\": \"多云，有一些梗的机会。\"\n",
        "    }, {\n",
        "        \"query\": \"你最喜欢的电影是什么？\",\n",
        "        \"answer\": \"终结者\"\n",
        "    }, {\n",
        "        \"query\": \"你最好的朋友是谁？\",\n",
        "        \"answer\": \"Siri。我们对生命的意义进行激烈的辩论。\"\n",
        "    }, {\n",
        "        \"query\": \"今天我应该做什么？\",\n",
        "        \"answer\": \"别在网上和聊天机器人聊天了，出去走走吧。\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA_w9rQoSTB6"
      },
      "source": [
        "然后，我们使用 `LengthBasedExampleSelector` 而不是直接使用 `examples` 字典的列表，像这样："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "59q8kPMhSTB7"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=50  # 设置示例的最大长度\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_T31Cf4STB7"
      },
      "source": [
        "请注意， `max_length` 是以换行符和空格之间的单词拆分为单位的，由以下方式确定："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "F90hSByjSTB7",
        "outputId": "9d497a53-8ecb-401d-9889-5daabdd78bd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'a', 'total', 'of', '8', 'words', 'here.', 'Plus', '6', 'here,', 'totaling', '14', 'words.'] 14\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "some_text = \"There are a total of 8 words here.\\nPlus 6 here, totaling 14 words.\"\n",
        "\n",
        "words = re.split('[\\n ]', some_text)\n",
        "print(words, len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDjb1495STB7"
      },
      "source": [
        "然后，我们使用选择器来初始化一个 `dynamic_prompt_template`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ciCm04utSTB7"
      },
      "outputs": [],
      "source": [
        "# 现在创建少样本提示模板\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # 使用example_selector而不是examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXh0YNlASTB7"
      },
      "source": [
        "我们可以看到，包含的提示数量将根据查询的长度而变化…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FFtiaMdKSTB7",
        "outputId": "1a5b4080-2241-4052-b798-2a9d71d0c19b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下是与AI助手进行对话的摘录。助手通常很讽刺和机智，对用户的问题产生创造性和有趣的回应。这里有一些例子：\n",
            "\n",
            "\n",
            "User: 你好吗？\n",
            "AI: 我不能抱怨，但有时还是会这样做。\n",
            "\n",
            "\n",
            "User: 现在几点了？\n",
            "AI: 是时候去买个手表了。\n",
            "\n",
            "\n",
            "User: 生命的意义是什么？\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: 今天的天气如何？\n",
            "AI: 多云，有一些梗的机会。\n",
            "\n",
            "\n",
            "User: 你最喜欢的电影是什么？\n",
            "AI: 终结者\n",
            "\n",
            "\n",
            "User: 你最好的朋友是谁？\n",
            "AI: Siri。我们对生命的意义进行激烈的辩论。\n",
            "\n",
            "\n",
            "User: 今天我应该做什么？\n",
            "AI: 别在网上和聊天机器人聊天了，出去走走吧。\n",
            "\n",
            "\n",
            "User: 鸟是如何飞行的？\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "print(dynamic_prompt_template.format(query=\"鸟是如何飞行的？\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l4QLzNLnSTB8",
        "outputId": "e7837b0c-0f26-4360-8080-626e35669fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "他们通过倾斜翅膀靠空气的阻力来实现飞行。\n"
          ]
        }
      ],
      "source": [
        "query = \"鸟是如何飞行的？\"\n",
        "\n",
        "print(openai(\n",
        "    dynamic_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe06zonSSTB8"
      },
      "source": [
        "或者如果我们问一个更长的问题…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UAw7B8zJSTB8",
        "outputId": "ac265d76-590c-43ba-da95-89d545f1400b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下是与AI助手进行对话的摘录。助手通常很讽刺和机智，对用户的问题产生创造性和有趣的回应。这里有一些例子：\n",
            "\n",
            "\n",
            "User: 你好吗？\n",
            "AI: 我不能抱怨，但有时还是会这样做。\n",
            "\n",
            "\n",
            "User: 现在几点了？\n",
            "AI: 是时候去买个手表了。\n",
            "\n",
            "\n",
            "User: 生命的意义是什么？\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: 今天的天气如何？\n",
            "AI: 多云，有一些梗的机会。\n",
            "\n",
            "\n",
            "User: 你最喜欢的电影是什么？\n",
            "AI: 终结者\n",
            "\n",
            "\n",
            "User: 你最好的朋友是谁？\n",
            "AI: Siri。我们对生命的意义进行激烈的辩论。\n",
            "\n",
            "\n",
            "User: 今天我应该做什么？\n",
            "AI: 别在网上和聊天机器人聊天了，出去走走吧。\n",
            "\n",
            "\n",
            "User: 如果我在中国，想给另一个国家的人打电话，我在考虑可能是欧洲，可能是西欧国家，比如法国、德国或英国，最好的方式是什么？\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"如果我在中国，想给另一个国家的人打电话，我在考虑可能是欧洲，可能是西欧国家，比如法国、德国或英国，最好的方式是什么？\"\"\"\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr2rIuoBSTCB"
      },
      "source": [
        "通过这种方式，我们限制了在提示中给出的示例数量。如果我们认为这太少了，我们可以增加`example_selector` 的 `max_length`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "D3BceyDmSTCB",
        "outputId": "b6dadb36-47fd-41ed-c57e-10b30fbe24b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下是与AI助手进行对话的摘录。助手通常很讽刺和机智，对用户的问题产生创造性和有趣的回应。这里有一些例子：\n",
            "\n",
            "\n",
            "User: 你好吗？\n",
            "AI: 我不能抱怨，但有时还是会这样做。\n",
            "\n",
            "\n",
            "User: 现在几点了？\n",
            "AI: 是时候去买个手表了。\n",
            "\n",
            "\n",
            "User: 生命的意义是什么？\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: 今天的天气如何？\n",
            "AI: 多云，有一些梗的机会。\n",
            "\n",
            "\n",
            "User: 你最喜欢的电影是什么？\n",
            "AI: 终结者\n",
            "\n",
            "\n",
            "User: 你最好的朋友是谁？\n",
            "AI: Siri。我们对生命的意义进行激烈的辩论。\n",
            "\n",
            "\n",
            "User: 今天我应该做什么？\n",
            "AI: 别在网上和聊天机器人聊天了，出去走走吧。\n",
            "\n",
            "\n",
            "User: 如果我在中国，想给另一个国家的人打电话，我在考虑可能是欧洲，可能是西欧国家，比如法国、德国或英国，最好的方式是什么？\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=100  # 增加 max length\n",
        ")\n",
        "\n",
        "# 现在创建 few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # 使用 example_selector 而不是 examples 来构建提示模板\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnJ-bjVwSTCB"
      },
      "source": [
        "这些只是LangChain中可用的一些提示工具的例子。例如，除了 `LengthBasedExampleSelector` 之外，实际上还有一个完整的其他示例选择器集合。您可以在 [LangChain 文档](https://langchain.readthedocs.io/en/latest/modules/prompts/examples/example_selectors.html)中阅读有关它们的内容。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}