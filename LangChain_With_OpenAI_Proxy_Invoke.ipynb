{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI0uPEexNHkz3D2d0Kyeov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crossme0809/langchain-tutorials/blob/main/LangChain_With_OpenAI_Proxy_Invoke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **如何在本地环境使用 `LangChain` 调用 `OpenAI` 代理（免科学上网）**"
      ],
      "metadata": {
        "id": "1ZBFWoLuTjED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai langchain"
      ],
      "metadata": {
        "id": "miLXtr5VySOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "HdcC36lJ3Tci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **方式一：通过在 `.env` 文件中设置 `OPENAI_API_BASE` 环境变量**\n"
      ],
      "metadata": {
        "id": "-cpxaWAjSpPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# 加载.env文件中的环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# 创建OpenAI实例\n",
        "llm = ChatOpenAI(\n",
        "        model_name=\"gpt-3.5-turbo-0613\",\n",
        "        max_tokens=1024,\n",
        "        verbose=True\n",
        "      )\n",
        "result = llm([HumanMessage(content=\"什么是LLM?\")])\n",
        "print(result.content)\n",
        "\n",
        "# 输出模型的参数信息可以看到已成功加载环境变量中OPENAI_API_BASE的值\n",
        "print(llm.json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEC6cavpPZBO",
        "outputId": "5d14d6f6-b5b0-4db6-978a-3a3705f62e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM是法学硕士（Master of Laws）的缩写。它是一个专业学位，主要面向已经获得法学学士学位或者相关学科学士学位的学生。LLM的学位课程通常涵盖法学的各个领域，如国际法、商法、民法、刑法等。这个学位旨在深入研究法学领域的专业知识和技能，提供更高水平的法律教育和培训。LLM学位在提升法律职业发展、进入法律界的国际化环境、深化法学研究等方面具有重要作用。\n",
            "<bound method BaseModel.json of ChatOpenAI(cache=None, verbose=True, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='https://gpt-proxy-3qe.pages.dev/api/v1', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1024)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **方式二：在初始化 `ChatOpenAI` 模型时指定 `openai_api_base` 参数**"
      ],
      "metadata": {
        "id": "Wi459Dj_TOhw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lih5m2o2u-cd",
        "outputId": "b9e75e21-9daf-4754-fbea-b437f4ba915a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM是法学硕士（Master of Laws）的缩写。它是一种研究生法学学位，通常是为那些已经获得法学学士学位的人提供的进一步学习和专业发展的机会。LLM课程通常侧重于深入研究法律领域的特定主题，如国际法、商法、人权法等。LLM学位可以在全球各地的大学和法学院获得。\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# 加载.env文件中的环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# 创建OpenAI实例\n",
        "llm = ChatOpenAI(\n",
        "        model_name=\"gpt-3.5-turbo-0613\",\n",
        "        max_tokens=1024,\n",
        "        verbose=True,\n",
        "        openai_api_base=os.getenv(\"OPENAI_API_BASE\")\n",
        "      )\n",
        "result = llm([HumanMessage(content=\"什么是LLM?\")])\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **基于`OpenAI Python SDK`调用`GPT`模型**"
      ],
      "metadata": {
        "id": "QXAy3JK8emEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo-0613\", messages=[{\"role\": \"user\", \"content\": \"请介绍下Google的PaLM模型!\"}])\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swUfRmguemM1",
        "outputId": "aef6f596-24e4-4120-ece1-466ef95286da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PaLM（Pretraining and Language Model）是Google在2020年提出的一种自然语言处理模型。PaLM的基本思想是将预训练和微调两个步骤结合起来，以提高模型的性能。\n",
            "\n",
            "PaLM模型的预训练阶段包括两个部分：语言建模（Language Modeling）和掩码语言建模（Masked Language Modeling）。语言建模通过让模型预测给定上下文下的下一个单词或token来学习语言的统计规律。而掩码语言建模则是在给定上下文中，随机掩盖一部分token，然后让模型预测被掩盖的token。这两个预训练任务有助于模型学习到单词和上下文之间的关系。\n",
            "\n",
            "在预训练之后，PaLM模型会进行微调以适应特定的下游任务，比如问答、文本分类等。微调阶段通过在特定任务上的有监督学习来调整模型的参数，使其更好地适应任务要求。\n",
            "\n",
            "PaLM模型在实验中表现出了良好的性能，它在自然语言理解和生成任务上都取得了较好的结果。与其他一些模型相比，PaLM模型具有良好的通用性和可扩展性，可以适应不同类型的下游任务。此外，PaLM模型还具有较强的泛化能力和抗噪能力。\n",
            "\n",
            "总结起来，Google的PaLM模型通过预训练和微调相结合的方式，提高了模型的性能，在自然语言处理任务中取得了良好的结果。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **通过`Python`直接调用`OpenAI HTTP`请求查询`Key`余额**"
      ],
      "metadata": {
        "id": "iNMyGYKWls7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://openai.1rmb.tk/v1/chat/completions\"\n",
        "api_key = 'sk-xxxxxxxxxxxxxxxxxxxx'\n",
        "\n",
        "headers = {\n",
        "  'Authorization': f'Bearer {api_key}',\n",
        "  'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "payload = {\n",
        "  \"model\": \"gpt-3.5-turbo\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Chroma和Pinecone的区别是什么？\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status() # 抛出异常，如果响应码不是200\n",
        "    data = response.json()\n",
        "    print(data)\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"请求错误: {e}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"无效的 JSON 响应: {e}\")"
      ],
      "metadata": {
        "id": "B0ihDXEDlZjW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}